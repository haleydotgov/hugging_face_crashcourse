{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a74f788",
   "metadata": {},
   "source": [
    "# Hugging Face Crash Course\n",
    "\n",
    "Pre-training and fine-tuning is one of the dominant paradgims in natural language processing. Here, models are developed in two phases\n",
    "\n",
    "1. train a large language model to develop generalizable language abilities\n",
    "2. add a classifier on top (typically) that you train to be good at your specific objective \n",
    "\n",
    "\n",
    "This notebook is meant to help you get started with Hugging Face and the transformers library. This library is fairly easy to use, but does provide a high level of abstraction. If you plan to dive deeper into Hugging Faces, I recommend brushing up on NLP concepts to make sure you understand what's going on under the hood.\n",
    "\n",
    "Also note that you will most likely run up against cloud.gov's memory limits when you install these packages and run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89a015e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e499da",
   "metadata": {},
   "source": [
    "## Auto Models\n",
    "\n",
    "Auto Models are a generic class that will be instantiated with the pre-trained model you specify. In other words, this gives you easy access to BERT, RoBERTA, LLAMA, and other powerful LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a46d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0570642b",
   "metadata": {},
   "source": [
    "For demonstration purposes, we're telling the Auto Model class to add a classification layer with two possible labels. ```NUM_LABELS``` can be set to an arbitrary number of classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d474788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6cb8c4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d1fc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "distilbert = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased', num_labels = NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "132cac96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f30fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "gpt = AutoModelForSequenceClassification.from_pretrained('openai-community/gpt2', num_labels = NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a9381f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce44693",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Pipelines are a quick way to use models for inference. These are helpful when you don't need to modify an off the shelf model, and just want to use it for inference. \n",
    "\n",
    "In addition to general-purpose LLMs, Hugging Face also hosts many models that have already been fine-tuned for specific tasks (e.g. sentiment classification, masked language modeling, text generation). \n",
    "\n",
    "Note that anyone can host a model on Hugging Face, make sure you know where your model is coming from, what kind of data it was trained on and its limitations.\n",
    "\n",
    "### Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfd3f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pipeline('text-classification', model = 'distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3396ce45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998213648796082}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment('I love natural language processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ec5f0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997296929359436}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment('I hate natural language processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef7afa",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efa060e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline('ner', model = 'dbmdz/bert-large-cased-finetuned-conll03-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76ed3e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-LOC',\n",
       "  'score': 0.99932003,\n",
       "  'index': 4,\n",
       "  'word': 'Washington',\n",
       "  'start': 10,\n",
       "  'end': 20},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9992067,\n",
       "  'index': 5,\n",
       "  'word': 'DC',\n",
       "  'start': 21,\n",
       "  'end': 23}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner('I live in Washington DC in an apartment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "984844ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99860793,\n",
       "  'index': 2,\n",
       "  'word': 'Joe',\n",
       "  'start': 10,\n",
       "  'end': 13},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9977203,\n",
       "  'index': 3,\n",
       "  'word': 'B',\n",
       "  'start': 14,\n",
       "  'end': 15},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9935376,\n",
       "  'index': 4,\n",
       "  'word': '##iden',\n",
       "  'start': 15,\n",
       "  'end': 19},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.9788887,\n",
       "  'index': 7,\n",
       "  'word': 'AI',\n",
       "  'start': 31,\n",
       "  'end': 33},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.9170989,\n",
       "  'index': 8,\n",
       "  'word': 'Executive',\n",
       "  'start': 34,\n",
       "  'end': 43},\n",
       " {'entity': 'I-MISC',\n",
       "  'score': 0.99059474,\n",
       "  'index': 9,\n",
       "  'word': 'Order',\n",
       "  'start': 44,\n",
       "  'end': 49}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner('President Joe Biden signed the AI Executive Order in October 2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5357b8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
